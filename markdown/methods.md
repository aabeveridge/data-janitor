##Methodology
As the introduction explains, improved access and greater transparency with data visualizations are limited in positively effecting data literacy if statistcal analyses are not understood as inventive, persuasive forms of argumentation. However, that does not mean that explaining methods and working to improve access is of lesser importance. Literacy and accessibility are always closely associated, and this is just as true in digital rhetoric as it was in print media. As James E. Porter argues, "From the standpoint of digital production, putting the concept of access into action means designing information so as to help audiences with limited access to digital resources engage that information via alternate media and formats" (216). But access in terms of data literacy is not as simple as access to data, but also includes the accessibility of methodologies and the technical know-how to unpack the production of data visualizations. Porter explains that technical knowledge "is integral to digital rhetoric, but that knowledge is not merely mechanical, routinized procedure. Yes, it can be reduced to that (and often is), but when practiced as art (*techne*) technical knowledge intersects with rhetorical and critical questions in order to assist discursive production and action" (220). In order for *techne* intersections of rhetoric and data visualizations to be possible, methodologies and underlying datasets must be made accessible. 

The following infographic summarizes the data and methods used to create the wordclouds in this article. This infographic was created at [easel.ly](http://easel.ly)---a site that provides free and easy-to-use cloud tools for creating, hosting, and embedding infographics. 

<a href="https://s3.amazonaws.com/easel.ly/all_easels/832990/wordcloud1/thumb.jpg"></a><img src="https://s3.amazonaws.com/easel.ly/all_easels/832990/wordcloud1/image.jpg" alt="wordcloud1" title="easel.ly" /></a>

The data visualizations produced in the [Data Janitor] and [Statistics] sections of this article rely on a form of text analysis called text mining or text data mining. Text mining takes raw unstructured text and turns it into structured data that can be statistically analyzed. For this article, a corpus of text documents is constructed from the following Wikipedia pages:

- [Composition Studies](https://en.wikipedia.org/wiki/Composition_studies)
- [Computers and Writing](https://en.wikipedia.org/wiki/Computers_and_writing)
- [Digital Rhetoric](https://en.wikipedia.org/wiki/Digital_rhetoric)
- [Ecocomposition](https://en.wikipedia.org/wiki/Ecocomposition)
- [English Studies](https://en.wikipedia.org/wiki/English_studies)
- [Media Studies](https://en.wikipedia.org/wiki/Media_studies)
- [Technical Communication](https://en.wikipedia.org/wiki/Technical_communication)
- [Visual Rhetoric](https://en.wikipedia.org/wiki/Visual_rhetoric)
- [Writing about Writing](https://en.wikipedia.org/wiki/Writing_about_Writing)
- [Writing Across the Curriculum](https://en.wikipedia.org/wiki/Writing_Across_the_Curriculum)

The text contained in the articles was systematically collected with a data mining application called [MassMine](http://massmine.org). MassMine is funded by the National Endowment for the Humanities, and is an open source software that supports social network data mining for academic research. MassMine was created to address limitations in accessibility to social network data[^1]. Some networks are more accessible than others in terms of how they license their data. Whereas the data from Twitter and Facebook data are strictly licenced and controlled, Wikipedia text data is open and accessible. But accessibility regarding data is more than licensing and terms of service---accessibility also has to do with the technical skills required to collect, analyze, and visualize data. The MassMine project works to address this problem by simplifying the process of collecting an archiving data from social networks. With Wikipedia, the creation of a corpus from a list of similar or associated articles is completed by providing MassMine with a list of article titles, and then all of the raw text is collected and organized for analysis. 

After MassMine collected the text data from Wikipedia, the data scrubbing and analysis was completed with the open source programming language called [R](https://cran.r-project.org/). R is designed specifically for data extraction and statistical analysis, and it is one of the best tools available for making informatics and data visualization methodologies widely accessible. Amanda Cox, the graphics editor for the *New York Times*, uses R to produce many of her data visualizations for the *Times*. Cox has said that R is "the greatest software on Earth," and she explains that while it is not the only tool she uses in her work, it is her preferred tool for "sketching" and exploring data when developing visual analyses. Although R can be used for more complex modeling and statistical predictions, Cox explains that its built in features remain flexible to fit many types of data and its package framework is friendly to new users who do not want to build all of their analyses from scratch. The *Data Stories* podcast has a recent interview with Cox [available here](http://datastori.es/ds-56-amanda-cox-nyt/#t=15:44.838), and *R-Bloggers* has videos of Cox talking about using R at the *Times* [available here](http://www.r-bloggers.com/amanda-cox-on-how-the-new-york-times-graphics-department-uses-r/). These resources provide real-world examples of how data analyses and visualizations are produced for a major media outlet.

All of the of the raw text data and the R code that produced the visualizations for this article are [available here](https://github.com/aabeveridge/data-janitor) on GitHub. GitHub is a free social coding site that provides support for open development and collaborative programming. Hopefully, by providing access to all of the code and text data that produced the visualizations below, it will allow other scholars and teachers to modify or build upon these examples and produce their own information rhetorics. Data literacy can be intimidating for rhetoric and writing scholars who have limited training in statistics and data visualization, but encouraging open and collaborative development will allow new projects to draw from and build on previous ones. As Mary K. Stewart argues, writing studies must continue to develop "a definition of digital literacy as a *learning outcome* that has three characterstics: multimodal composition, information, and collaboration." As GitHub is quickly becoming the largest social coding site on the web, and it provides an open and collaborative framework for development that supports the learning outcomes Stewart describes. 

[^1]: MassMine grant details: http://ufdc.ufl.edu/AA00025642/00001
